{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MXNET_CUDNN_LIB_CHECKING\"] = \"0\"\n",
    "os.environ[\"MXNET_CUDNN_AUTOTUNE_DEFAULT\"] = \"0\"\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import  autograd, context\n",
    "from mxnet.base import MXNetError\n",
    "from mxnet.gluon.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import myModel\n",
    "from image_Dictionary import ImageDict\n",
    "import const\n",
    "from my_Save import saveAsCSV, SaveModels\n",
    "from mx_Train import myTrain\n",
    "from decode.postprocessing.instance_segmentation import InstSegm\n",
    "from myPlots import lossPlot, visualize_all, plotPredictedImape\n",
    "from iou import  get_iou\n",
    "from image_Augmentor import GeoTiffDataset, SatelliteImageAugmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set image type VNIR or NDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isVnir = True\n",
    "imageType = \"NDV\"\n",
    "if isVnir:\n",
    "    imageType = \"VNIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx=context.gpu()\n",
    "mx.nd.waitall()\n",
    "numberOfimages =648\n",
    "\n",
    "input_directory = const.images_2022\n",
    "output_directory = os.path.join(const.result_2022_aug, imageType, str(numberOfimages))\n",
    "output_models= os.path.join(output_directory,\"models\")\n",
    "result_path = os.path.join(output_directory, \"result\")\n",
    "lossFile =   os.path.join(output_directory,\"loss.csv\") \n",
    "\n",
    "def makedir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(path)\n",
    "    \n",
    "for i in [output_directory, output_models, result_path]:\n",
    "    makedir(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Mask Loading with Preprocessing\n",
    "\n",
    "The following code is used to load satellite images and their corresponding masks, with an option to preprocess them by selecting all available bands or just the standard three (e.g., RGB).\n",
    "\n",
    "# Parameters\n",
    "\n",
    "isAllband:\n",
    "\n",
    "True: Loads all available spectral bands of the image.\n",
    "\n",
    "False: Loads only the three standard bands (typically RGB).\n",
    "\n",
    "numberOfimages:\n",
    "Specifies how many images to load.\n",
    "\n",
    "Set this to control the size of your dataset for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_2022= ImageDict(const.images_2022,  False)\n",
    "image_dict_2022  = images_2022.load_tif_files(imageType, numberOfimages = numberOfimages, isAllband= True)\n",
    "masks_2022 = ImageDict(const.masks_2022, True)\n",
    "mask_dict_2022  = masks_2022.load_tif_files(imageType, image_dict_2022, numberOfimages = numberOfimages, isAllband= True)\n",
    "train_ids, val_ids = train_test_split(list(mask_dict_2022.keys()), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_2010 = ImageDict(const.images_2010, False)\n",
    "testimages = 648\n",
    "image_dict_2010 = images_2010.load_tif_files(imageType, numberOfimages=testimages, isAllband= True)\n",
    "output_directory_2010 = os.path.join(const.result_2022_3, imageType)\n",
    "makedir(output_directory_2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Images and Masks\n",
    "Extract image data from a dictionary based on a list of IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(ids, image_dict):\n",
    "    data=  np.array([image_dict[id].image for id in ids])\n",
    "    return mx.nd.array(data)\n",
    "\n",
    "train_images =get_images(train_ids, image_dict_2022)\n",
    "train_masks = get_images(train_ids, mask_dict_2022)\n",
    "val_images = get_images(val_ids, image_dict_2022)\n",
    "val_masks = get_images(val_ids, mask_dict_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batched data loaders for training and validation using MXNet's DataLoader, enabling efficient mini-batch processing\n",
    "\n",
    "If you are using without hyperparameter tuning or Mixup and Cutmix please this below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "try:\n",
    "    train_dataset = mx.gluon.data.ArrayDataset(train_images, train_masks)\n",
    "    train_loader = mx.gluon.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0,shuffle=True)\n",
    "    val_loader = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(val_images, val_masks), batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating data loaders: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters the training data from the full dataset, applies data augmentation using SatelliteImageAugmentor, and prepares custom and standard data loaders for training and validation.\n",
    "Use this you are using the Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatImgDict(ids , dict):\n",
    "    newdict = {}\n",
    "    for i in ids:\n",
    "        newdict[i] = dict[i]\n",
    "    return newdict\n",
    "    \n",
    "new_image_dict_2022 = creatImgDict(train_ids, image_dict_2022)\n",
    "new_mask_dict_2022 = creatImgDict(train_ids, mask_dict_2022)\n",
    "\n",
    "# Initialize Augmentor and Dataset\n",
    "augmentor = SatelliteImageAugmentor()\n",
    "train_dataset = GeoTiffDataset(new_image_dict_2022, new_mask_dict_2022, augmentor=augmentor)\n",
    "print('train_dataset', len(train_dataset))\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(val_images, val_masks), batch_size=batch_size, num_workers=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training section\n",
    "\n",
    "Models per epoch are saved.\n",
    "\n",
    "Validation and Training loss per epoch are saved in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of retries\n",
    "max_retries = 5\n",
    "retry_count = 0\n",
    "\n",
    "# Retry logic\n",
    "while retry_count < max_retries:\n",
    "    try:\n",
    "        mxTn = myTrain(train_loader, val_loader)\n",
    "        loss_each_epoch, model_list, epoch = mxTn.train(ctx, epochs = 50)\n",
    "        saveAsCSV([\"Current Epoch\", \"Training Loss\", \"Validation Loss\"], lossFile, loss_each_epoch)\n",
    "        SaveModels(output_models, model_list)\n",
    "        lossPlot(loss_each_epoch, output_directory)\n",
    "        break\n",
    "    except MXNetError  as e:\n",
    "        if 'CUDNN_STATUS_EXECUTION_FAILED' in str(e):\n",
    "            print(f\"cuDNN execution failed. Retrying... ({retry_count + 1}/{max_retries})\")\n",
    "            mx.nd.waitall()  # Clear GPU memory\n",
    "            time.sleep(5) # Wait for a few seconds before retrying\n",
    "            retry_count += 1 # Increment the retry counter\n",
    "        else:\n",
    "            raise  # If it's another error, raise it\n",
    "\n",
    "# Check if maximum retries were reached\n",
    "if retry_count == max_retries:\n",
    "    print(\"Maximum retries reached. Training failed due to cuDNN error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the most recent model checkpoint file from the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_file_name():\n",
    "    files = os.listdir(output_models)  # Get all files in the folder\n",
    "    if files:\n",
    "        last_file = os.path.join(output_models, f'model_VNIR_{len(files)-1}.params')\n",
    "        print(f\"This model is using: {last_file}\")\n",
    "        return last_file\n",
    "    else:\n",
    "        print(\"The folder is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads an image and its corresponding mask (if available) based on the year (2022 or 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_metadata(id, is2022):\n",
    "    if is2022:\n",
    "        img = images_2022.getImage(id, image_dict_2022, ctx)\n",
    "        mask = masks_2022.getImage(id, mask_dict_2022, ctx)\n",
    "        currentMetadata = image_dict_2022[id]\n",
    "    else:\n",
    "        img = images_2010.getImage(id, image_dict_2010, ctx)\n",
    "        print('img:', img.shape)\n",
    "        currentMetadata = image_dict_2010[id]\n",
    "        print('currentMetadata:', currentMetadata.image.shape)\n",
    "        mask = None\n",
    "    return img, mask, currentMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the appropriate reference shapefile path based on the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_path(is2022):\n",
    "    ref_path = const.output_ref_2022    \n",
    "    if not is2022:\n",
    "        ref_path = const.output_ref_2010\n",
    "    return ref_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Loads the model and performs predictions on a list of image IDs.\n",
    "   - Outputs: segmentation, boundaries, distances, and instance masks.\n",
    "   - Visualizes results, saves prediction shapefiles, and computes IoU scores.\n",
    "   - All outputs (visuals + metrics) are stored in `result_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(result_path, val_ids, t_ext , t_bound , is2022):    \n",
    "    print(f\"Starting visualization with t_ext = {t_ext}, t_bound = {t_bound}\")\n",
    "    modelPath = rf\"{get_model_file_name()}\"\n",
    "    ref_path = get_ref_path(is2022)   \n",
    "\n",
    "    netPredict = myModel.MyFractalResUNetcmtsk(True, modelPath, ctx)\n",
    "    ious=[]\n",
    "    ious.append({\"ID\": f't_ext: {t_ext}',\"IOU\": f't_bound: {t_bound}'}) \n",
    "    plotColl = []\n",
    "\n",
    "    for id in val_ids:  # Limit to 'num_images' for visualization\n",
    "        print(f\"Processing image ID: {id}\")\n",
    "        try:\n",
    "            img, mask, currentMetadata = get_img_metadata(id, is2022)\n",
    "            with autograd.predict_mode():  \n",
    "                outputs = netPredict.net(img) \n",
    "                print(f\"I am here debugging: {id}\")\n",
    "                pred_segm  = np.array(outputs[0][0,1,:,:].asnumpy())\n",
    "                pred_bound =  np.array(outputs[1][0,1,:,:].asnumpy())\n",
    "                pred_dists =  np.array(outputs[2][0,1,:,:].asnumpy()) \n",
    "                pred_segm = 1-pred_segm\n",
    "                inst =InstSegm(pred_segm, pred_bound, t_ext=t_ext, t_bound=t_bound)   # perform instance segmentation\n",
    "                inst = np.nan_to_num(inst, nan=0)\n",
    "                if is2022:\n",
    "                    imgColl = plotPredictedImape(id, img, mask, pred_segm, pred_bound, pred_dists, inst, ref_path)\n",
    "                    plotColl.append(imgColl)\n",
    "                output_shapefile_path = visualize_all(id, img, currentMetadata, outputs, pred_segm, pred_bound, inst, result_path)\n",
    "                print(\"Start IOU calculation\")\n",
    "                csv_file_path = os.path.join(result_path, str(id), \"iou.csv\")\n",
    "                iou_score= get_iou(os.path.join(ref_path, f'tile_{id}.shp'), os.path.join(output_shapefile_path, f'{str(id)}.shp'))\n",
    "                ious.append({ \"ID\": id,\"IOU\": iou_score })\n",
    "                saveAsCSV([\"ID\", \"IOU\"], csv_file_path, ious, True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image ID {id}: {e}\")\n",
    "    \n",
    "    return plotColl\n",
    "       \n",
    "def visualize(result_path, val_ids,  t_ext , t_bound , is2022 = True):\n",
    "    random_val_ids = random.choice(val_ids) # Choose a random validation ID\n",
    "    return visualize_predictions(result_path, val_ids ,t_ext = t_ext, t_bound = t_bound, is2022 = is2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying model in 2022 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for vnir: t_ext = 0.6, t_bound = 0.1\n",
    "#6612\n",
    "results_2022 = visualize(result_path, val_ids, t_ext = 0.6, t_bound = 0.1, is2022 = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying model in 2010 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_directory_2010)\n",
    "#list(image_dict_2010.keys())\n",
    "results_2010 =visualize(output_directory_2010,[206] ,  t_ext = 0.6, t_bound = 0.1, is2022= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satellite_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
